{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z2HLdaVjIh7P"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/huggingface/diffusers\n",
        "! pip install -U -r requirements.txt\n",
        "! pip install bitsandbytes\n",
        "! pip install torch-fidelity\n",
        "! pip uninstall torch torchvision torchaudio\n",
        "! pip install torch torchvision torchaudio\n",
        "! pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9yHqaCVJLFy"
      },
      "outputs": [],
      "source": [
        "! accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGMd7zTIKuh4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as F\n",
        "from torchmetrics.image.kid import KernelInceptionDistance\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers.utils import make_image_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "slGdkrqEdqlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQPu3dAeJ939"
      },
      "outputs": [],
      "source": [
        "! accelerate launch train_dreambooth.py --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" --instance_data_dir=\"/content/Kamao\" --output_dir=\"/content/DB/\" --instance_prompt=\"a photo of Kamao\" --resolution=512 --gradient_checkpointing --use_8bit_adam --train_batch_size=1 --gradient_accumulation_steps=1 --learning_rate=5e-6 --lr_scheduler=\"constant\" --lr_warmup_steps=0 --max_train_steps=500"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Generation"
      ],
      "metadata": {
        "id": "AYgmfUBNd6YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hfpMaaA0LeMZ"
      },
      "outputs": [],
      "source": [
        "model_id = \"/content/DB/\"\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
        "\n",
        "prompt = \"a photo of Kamao\"\n",
        "images = pipe(prompt, num_inference_steps=50, guidance_scale=7.5, num_images_per_prompt=16).images\n",
        "make_image_grid(images[:8], rows=1, cols=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating KID"
      ],
      "metadata": {
        "id": "D7Sw2tqcecAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjDpRxivQRvO"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    image = torch.tensor(image).unsqueeze(0)\n",
        "    image = image.permute(0, 3, 1, 2) / 255.0\n",
        "    return image\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "dataset_path = \"/content/Kamao\"\n",
        "image_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])\n",
        "real_images = [np.array(Image.open(path).convert(\"RGB\")) for path in image_paths]\n",
        "real_images = torch.cat([preprocess_image(image) for image in real_images])\n",
        "\n",
        "fake_images = images\n",
        "fake_images = torch.cat([transform(image).unsqueeze(0) for image in fake_images])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuwrnMG6SfpM"
      },
      "outputs": [],
      "source": [
        "kid = KernelInceptionDistance(normalize=True, subset_size=8)\n",
        "kid.update(real_images, real=True)\n",
        "kid.update(fake_images, real=False)\n",
        "\n",
        "print(f\"KID: {kid.compute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note to grader: due to a technical error with saving, the code for calculating CLIP scores is lost."
      ],
      "metadata": {
        "id": "9iMAgsaSfELc"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}